# ğŸ›¢ Youtube-Data-Streaming-Pipeline

A real-time data streaming application that extracts data from multiple sources, processes it through Kafka and Apache Spark, and stores it in an S3 bucket for further analysis.

![Architecture](./docs/pineline.png)

## ğŸ“– Table of Contents
- [âš™ï¸ Features](#ï¸-features)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Installation](#ï¸-installation)
- [ğŸ¤ Contributing](#ï¸-contributing)
- [ğŸ“ƒ License](#ï¸-license)

## âš™ï¸ Features

- **Multi-Source Data Extraction**: Collects data from YouTube API, Spotify API, and OpenWeather API.
- **Real-Time Data Processing**: Utilizes Kafka for message streaming and Apache Spark for data processing.
- **Scalable Architecture**: Runs in a containerized environment using Docker.

## ğŸ› ï¸ Tech Stack  

This project utilizes the following technologies:

- **Data Sources**: YouTube API, Spotify API, OpenWeather API
- **Streaming & Processing**: Apache Kafka, Apache Spark
- **Containerization**: Docker
- **Storage**: Amazon S3
- **Prompting**: LLM Model

## ğŸš€ Installation

Follow the steps below to run the project locally:

### 1. Clone the Repository  

```bash
git clone https://github.com/PhamAnhTienn/Youtube-Data-Streaming-Pipeline.git
cd Youtube-Data-Streaming-Pipeline
```

### 2. Build and Start the Docker Containers
Ensure Docker is installed, then run:

```bash
docker-compose up --build
```

### 3. Access the Application
Once the setup is complete, start the data streaming process:

```bash
cd jobs
python main.py
```

## ğŸ¤ Contributing

This project is open to contributions. Please feel free to submit a PR.

## ğŸ“ƒ License

This project is provided under an MIT license. See the [LICENSE](LICENSE) file for details.